{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer, Conv2D, BatchNormalization, Concatenate, UpSampling2D, MaxPool2D, Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import HAR_utils\n",
    "from HAR_utils import get_activity_index, plot_confusion_matrix, get_Train_Test, timeseries_standardize\n",
    "from attention import Attention2D\n",
    "\n",
    "fig_width = 12\n",
    "plt.rcParams[\"font.size\"] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Jaime\\\\Documents\\\\JaimeMorales\\\\Codes\\\\HAR_acc\\\\acc2\\\\daily_data\\\\acc_03_r.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b0366530c90d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_03_r.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Jaime\\\\Documents\\\\JaimeMorales\\\\Codes\\\\HAR_acc\\\\acc2\\\\daily_data\\\\acc_03_r.csv'"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_03_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_04_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_05_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_06_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(['LW_x','LW_y','LW_z'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY,testtimes,selector = get_Train_Test(data_df,trn=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_df[data_df['job']==selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainX(shape): ',trainX.shape)\n",
    "print('trainY(shape): ',trainY.shape)\n",
    "\n",
    "print('testX(shape): ',testX.shape)\n",
    "print('testY(shape): ',testY.shape)\n",
    "print('testtimes(len): ',len(testtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = trainX[[5]]\n",
    "trx = np.delete(trainX,5,0)\n",
    "print('trainX(shape): ',tex.shape)\n",
    "print('trainX(shape): ',trx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX , testX = timeseries_standardize(trainX, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"down_\"):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    p = MaxPool2D((1, 2),name=name+\"M\")(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"up_\"):\n",
    "    us = UpSampling2D((1, 2),name=name+\"U\")(x)\n",
    "    concat = Concatenate()([us, skip])\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(concat)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl2\")(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    return c\n",
    "\n",
    "def build_unet_model(height,channels,class_num,f=32):\n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    c2, p2 = down_block(p1, f*2, name=\"down_2_\")\n",
    "    c3, p3 = down_block(p2, f*4, name=\"down_3_\")\n",
    "    p3 = Dropout(0.5)(p3)\n",
    "    \n",
    "    bn = bottleneck(p3, f*8)\n",
    "    \n",
    "    u1 = up_block(bn, c3, f*4, name=\"up_1_\")\n",
    "    u2 = up_block(u1, c2, f*2, name=\"up_2_\")\n",
    "    u3 = up_block(u2, c1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def ApplyUnet(trainX, trainY, testX, testY,TestTimes, activities, nb_epoch=100, fi=32):\n",
    "    \n",
    "    target_names,indices = get_activity_index(activities)\n",
    "    #print(target_names)\n",
    "    indices.append(11)\n",
    "    target_names.append('EPadding')\n",
    "    \n",
    "    trainY_b = np_utils.to_categorical(trainY, len(target_names))\n",
    "    testY_b = np_utils.to_categorical(testY, len(target_names))\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    print(indices)\n",
    "    lb.fit(indices)\n",
    "    \n",
    "    model = build_unet_model(trainX.shape[2],trainX.shape[3],len(target_names),f=fi)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "    history = model.fit(trainX, trainY_b, epochs=nb_epoch, validation_split=0.2,callbacks=[model_checkpoint], verbose=0)\n",
    "    \n",
    "    fig, (tr_plt,cm_plt) = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
    "    \n",
    "    tr_plt.plot(history.history['loss'])\n",
    "    tr_plt.plot(history.history['val_loss'])\n",
    "    tr_plt.set_title('Model loss')\n",
    "    tr_plt.set_ylabel('Loss')\n",
    "    tr_plt.set_xlabel('Epoch')\n",
    "    tr_plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "      \n",
    "    predict_v = model.predict(testX)\n",
    "    predict_vec = predict_v[0][0]\n",
    "    confidences = np.max(predict_vec, axis=1)\n",
    "    testYY = testY[0][0]\n",
    "    \n",
    "    predict_class = np.argmax(predict_vec, axis=1)\n",
    "    predict = [lb.classes_[x] for x in predict_class]\n",
    "    \n",
    "    print(\"len(testYY)\",len(testYY))\n",
    "    print(\"len(predict)\",len(predict))\n",
    "    \n",
    "    testXX = testX[0][0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(len(testYY)):\n",
    "        if testYY[i]==11:\n",
    "            testYYY = testYY[:i]\n",
    "            predict1 = predict[:i]\n",
    "            testXXX = testXX[:i]\n",
    "            ttimes = TestTimes[:i]\n",
    "            Confidences = confidences[:i]\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "    \n",
    "    '''\n",
    "    testYYY = testYY\n",
    "    predict1 = predict\n",
    "    testXXX = testXX\n",
    "    ttimes = TestTimes\n",
    "    Confidences = confidences\n",
    "    '''\n",
    "              \n",
    "    print(\"len(testYYY)\",len(testYYY))\n",
    "    print(\"len(predict1)\",len(predict1))\n",
    "    \n",
    "    result = classification_report(testYYY, predict1, labels=indices, target_names=target_names)\n",
    "    print(result)\n",
    "    testXX=testX\n",
    "    cnf_matrix = confusion_matrix(testYYY, predict1)\n",
    "    plot_confusion_matrix(cnf_matrix, fig, cm_plt, classes=target_names)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    Sloss = history.history['loss']\n",
    "\n",
    "    return predict1, testYYY, testXXX, Confidences, ttimes, Sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclass_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\TF-env\\projects\\U-net\\acc2\\labels.csv')\n",
    "simclass_d = simclass_df['activity'].to_dict()\n",
    "#simclass_d[11]='pad_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict1, testYYY, testXXX, Confidences, Ttimes, Sloss = ApplyUnet(trainX, trainY, testX, testY, testtimes, simclass_d, nb_epoch=100, fi=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred = pd.DataFrame(data = {'time':Ttimes, 'l_val':testYYY, 'predicted_label':predict1,'confidence':Confidences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred.to_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\results\\Test-03-09\\U-net\\6\\res_6_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tttimes = range(len(predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels,indices = get_activity_index(simclass_d)\n",
    "indices.append(11)\n",
    "labels.append('EPadding')\n",
    "figs = plt.figure('result',figsize=(10, 4))\n",
    "axs = plt.subplot(111)\n",
    "axs.plot(tttimes, predict1, color='r')\n",
    "axs.plot(tttimes, testYYY, color='g')\n",
    "axs.set_ylabel('Activity label')\n",
    "axs.set_yticks(indices)\n",
    "axs.set_yticklabels(labels)\n",
    "figs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
