{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer,concatenate, Conv2D, BatchNormalization, Concatenate, UpSampling2D, MaxPool2D, Input, Dropout, Dense, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, normalize\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "fig_width = 12\n",
    "plt.rcParams[\"font.size\"] = 40\n",
    "plt.rcParams['axes.labelsize'] = 40\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "#plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "#plt.rcParams.keys()\n",
    "\n",
    "from codes.utils.HAR_utils import get_activity_index, plot_confusion_matrix, get_Train_Test, timeseries_standardize\n",
    "from codes.models.attention import Attention2D\n",
    "#from getattention import get_intermediate_output, write_intermediate_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_bird(in_df,shuffle=False,trn='A'):\n",
    "    \n",
    "    in_df.dropna(inplace=True)\n",
    "    birds = list(in_df.drop_duplicates(subset=['animal_tag'],keep = 'first')['animal_tag'])\n",
    "    max_len = 0\n",
    "\n",
    "    for bird in birds:\n",
    "        c_len = len(in_df[in_df['animal_tag'] == bird])\n",
    "        \n",
    "        if c_len > max_len:\n",
    "            print(bird,' : ',c_len,' > ',max_len,' ? ',c_len > max_len)\n",
    "            max_len = c_len\n",
    "        print('bird: ',bird,'lenght: ',c_len)\n",
    "    reqlen = int((int(max_len/512)+1)*512)\n",
    "    print('reqlen: ',reqlen)\n",
    "\n",
    "    features = in_df.columns[2:-2]\n",
    "    X = []\n",
    "    Y = []\n",
    "    times = []\n",
    "    normalizer = Normalizer()\n",
    "\n",
    "    for bird in birds:\n",
    "            x = np.array(in_df[in_df['animal_tag'] == bird][features].values)\n",
    "            #print('x.shape',x.shape)\n",
    "            xz = np.zeros((reqlen-len(x),len(features)))\n",
    "            xs = normalizer.fit_transform(x)\n",
    "            sx = np.concatenate((x,xz))\n",
    "    \n",
    "            y = np.array(in_df[in_df['animal_tag'] == bird]['l_val'].values)\n",
    "            z = np.array([10]*(reqlen-len(y)))\n",
    "            sy = np.concatenate((y,z))\n",
    "    \n",
    "            t = np.array(in_df[in_df['animal_tag'] == bird]['timestamp'].values)\n",
    "            st = np.concatenate((t,z))\n",
    "    \n",
    "            X.append([sx])\n",
    "            Y.append([sy])\n",
    "            times.append(st)\n",
    "\n",
    "    if trn != 1000:\n",
    "        i=0\n",
    "        if shuffle == True:\n",
    "            shuffle == False\n",
    "            print(\"Warning: shuffle cannot be used when selecting a specific bird\")\n",
    "        for bird in birds:\n",
    "            if bird == trn:\n",
    "                selector = i\n",
    "            i=1+i\n",
    "    else:\n",
    "        selector = random.randint(0,len(birds)-1)\n",
    "\n",
    "    if shuffle:\n",
    "        p = list(zip(X,Y))\n",
    "        random.shuffle(p)\n",
    "        X, Y = zip(*p)\n",
    "        X = list(X)\n",
    "        Y = list(Y)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    times = np.array(times)\n",
    "    print('X(shape): ',X.shape)\n",
    "    print('Y(shape): ',Y.shape)\n",
    "    print('times(shape): ',times.shape)\n",
    "\n",
    "    testX = np.array([X[selector]])\n",
    "    testY = np.array([Y[selector]])\n",
    "    testtimes=times[selector]\n",
    "    trainX = np.delete(X,selector,0)\n",
    "    trainY = np.delete(Y,selector,0)\n",
    "    \n",
    "    return trainX,testX,trainY,testY,testtimes,selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(ip,filters=10):\n",
    "    depth_k = 10\n",
    "    depth_v = 10\n",
    "    num_heads = 1\n",
    "    qkv_conv = Conv2D(2 * depth_k + depth_v, (1, 1), strides=(1, 1), padding='same', use_bias=True, kernel_initializer='he_normal', name=\"pre-qkvcon\")(ip)\n",
    "    attn_out = Attention2D(depth_k, depth_v, num_heads, True, name=\"attn\")(qkv_conv)\n",
    "    attn_out = Conv2D(depth_v, (1, 1), strides=(1, 1), padding='same', use_bias=True, kernel_initializer='he_normal', name=\"pos-qkvcon\")(attn_out)\n",
    "    att_output = Multiply()([ip, attn_out])\n",
    "    #att_output = BatchNormalization()(att_output)\n",
    "    return att_output\n",
    "\n",
    "def down_block(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"down_\"):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    p = MaxPool2D((1, 4),name=name+\"M\")(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"up_\"):\n",
    "    us = UpSampling2D((1, 4),name=name+\"U\")(x)\n",
    "    concat = Concatenate()([us, skip])\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(concat)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl2\")(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    return c\n",
    "\n",
    "def build_attunet_model(height,channels,class_num,f=32):\n",
    "    \n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    att1 = attention_block(c1, filters=f)\n",
    "    pa = MaxPool2D((1, 4))(att1)\n",
    "    \n",
    "    \n",
    "    bn = bottleneck(pa, f*2)\n",
    "    \n",
    "    u3 = up_block(bn, att1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    '''\n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    att1 = attention_block(c1, filters=f)\n",
    "    pa = MaxPool2D((1, 8))(att1)\n",
    "    \n",
    "    \n",
    "    bn = bottleneck(pa, f*2)\n",
    "    \n",
    "    u3 = up_block(bn, att1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    att1 = attention_block(c1, filters=f)\n",
    "    pa = MaxPool2D((1, 8))(att1)\n",
    "    c2, p2 = down_block(pa, f*2, name=\"down_2_\")\n",
    "    \n",
    "    bn = bottleneck(p2, f*4)\n",
    "    \n",
    "    u2 = up_block(bn, c2, f*2, name=\"up_2_\")\n",
    "    u3 = up_block(u2, att1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    att1 = attention_block(c1, filters=f)\n",
    "    pa = MaxPool2D((1, 8))(att1)\n",
    "    c2, p2 = down_block(pa, f*2, name=\"down_2_\")\n",
    "    c3, p3 = down_block(p2, f*4, name=\"down_3_\")\n",
    "    p3 = Dropout(0.5)(p3)\n",
    "    \n",
    "    bn = bottleneck(p3, f*8)\n",
    "    \n",
    "    u1 = up_block(bn, c3, f*4, name=\"up_1_\")\n",
    "    u2 = up_block(u1, c2, f*2, name=\"up_2_\")\n",
    "    u3 = up_block(u2, att1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    att1 = attention_block(c1, filters=f)\n",
    "    pa = MaxPool2D((1, 4))(att1)\n",
    "    c2, p2 = down_block(pa, f*2, name=\"down_2_\")\n",
    "    c3, p3 = down_block(p2, f*4, name=\"down_3_\")\n",
    "    c4, p4 = down_block(p3, f*8, name=\"down_4_\")\n",
    "    p4 = Dropout(0.5)(p4)\n",
    "    \n",
    "    bn = bottleneck(p4, f*16)\n",
    "    \n",
    "    u1 = up_block(bn, c4, f*8, name=\"up_1_\")\n",
    "    u2 = up_block(u1, c3, f*4, name=\"up_2_\")\n",
    "    u3 = up_block(u2, c2, f*2, name=\"up_3_\")\n",
    "    u4 = up_block(u3, att1, f, name=\"up_4_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u4)\n",
    "    model = Model(inputs, outputs)\n",
    "    '''\n",
    "    \n",
    "    return model\n",
    "\n",
    "def ApplyUnet(trainX, trainY, testX, testY,TestTimes, activities, nb_epoch=100, fi=32):\n",
    "    \n",
    "    target_names,indices = get_activity_index(activities)\n",
    "    #print(target_names)\n",
    "    indices.append(len(target_names))\n",
    "    target_names.append('EPadding')\n",
    "    \n",
    "    trainY_b = np_utils.to_categorical(trainY, len(target_names))\n",
    "    testY_b = np_utils.to_categorical(testY, len(target_names))\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    print(indices)\n",
    "    lb.fit(indices)\n",
    "    \n",
    "    model = build_attunet_model(trainX.shape[2],trainX.shape[3],len(target_names),f=fi)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint('bioattunet-2022_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "    history = model.fit(trainX, trainY_b, epochs=nb_epoch, validation_split=0.2,callbacks=[model_checkpoint], verbose=0)\n",
    "    \n",
    "    fig, (tr_plt,cm_plt) = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
    "    \n",
    "    tr_plt.plot(history.history['loss'])\n",
    "    tr_plt.plot(history.history['val_loss'])\n",
    "    tr_plt.set_title('Model loss')\n",
    "    tr_plt.set_ylabel('Loss')\n",
    "    tr_plt.set_xlabel('Epoch')\n",
    "    tr_plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "\n",
    "      \n",
    "    predict_v = model.predict(testX)\n",
    "    predict_vec = predict_v[0][0]\n",
    "    confidences = np.max(predict_vec, axis=1)\n",
    "    testYY = testY[0][0]\n",
    "    \n",
    "    predict_class = np.argmax(predict_vec, axis=1)\n",
    "    predict = [lb.classes_[x] for x in predict_class]\n",
    "    \n",
    "    print(\"len(testYY)\",len(testYY))\n",
    "    print(\"len(predict)\",len(predict))\n",
    "    \n",
    "    testXX = testX[0][0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(len(testYY)):\n",
    "        #print(testYY[i])\n",
    "        if testYY[i]==(len(target_names)-1):\n",
    "            testYYY = testYY[:i]\n",
    "            predict1 = predict[:i]\n",
    "            testXXX = testXX[:i]\n",
    "            ttimes = TestTimes[:i]\n",
    "            Confidences = confidences[:i]\n",
    "            print(testYY[i])\n",
    "            print(i)\n",
    "            break\n",
    "        else:\n",
    "            testYYY = testYY\n",
    "            predict1 = predict\n",
    "            testXXX = testXX\n",
    "            ttimes = TestTimes\n",
    "            Confidences = confidences\n",
    "\n",
    "              \n",
    "    print(\"len(testYYY)\",len(testYYY))\n",
    "    print(\"len(predict1)\",len(predict1))\n",
    "    \n",
    "    result = classification_report(testYYY, predict1, labels=indices, target_names=target_names)\n",
    "    print(result)\n",
    "    testXX=testX\n",
    "    cnf_matrix = confusion_matrix(testYYY, predict1)\n",
    "    plot_confusion_matrix(cnf_matrix, fig, cm_plt, classes=target_names)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    Sloss = history.history['loss']\n",
    "\n",
    "    return predict1, testYYY, testXXX, Confidences, ttimes, Sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run leave_one_bird_out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_bird_out_cv(data_df,save_folder,simclass_d, nb_epoch=200, fi=10):\n",
    "\n",
    "    data_df.drop(['year'],axis=1,inplace=True)\n",
    "    data_df = data_df[data_df['label']!='unlabeled']\n",
    "    birds = list(data_df.drop_duplicates(subset=['animal_tag'],keep = 'first')['animal_tag'])\n",
    "\n",
    "    for bird in birds:\n",
    "        trainX,testX,trainY,testY,testtimes,selector = get_train_test_bird(data_df,trn=bird)\n",
    "        trainX , testX = timeseries_standardize(trainX, testX)\n",
    "        predict1, testYYY, testXXX, Confidences, Ttimes, Sloss = ApplyUnet(trainX, trainY, testX, testY, testtimes, simclass_d, nb_epoch=nb_epoch, fi=fi)\n",
    "        test_data_with_conf_pred = pd.DataFrame(data = {'timestamp':Ttimes, 'l_val':testYYY, 'predicted_label':predict1,'confidence':Confidences})\n",
    "        save_fn = 'res_'+bird+'.csv'\n",
    "        save_path = os.path.join(save_folder,save_fn)\n",
    "        test_data_with_conf_pred.to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\omizunagidori\\2022\\l_2022_acc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclass_df = pd.read_csv(r'G:\\JaimeMorales\\Codes\\omizunagidori\\database\\labels\\O_labels_df.csv')\n",
    "simclass_d = simclass_df['label'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\results\\baselines\\att-Unet\\U_2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_bird_out_cv(data_df,save_folder,simclass_d, nb_epoch=200, fi=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run induvidual test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\omizunagidori\\2019\\l_2019_acc.csv\")\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(['year'],axis=1,inplace=True)\n",
    "data_df = data_df[data_df['label']!='unlabeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = list(data_df.drop_duplicates(subset=['animal_tag'],keep = 'first')['animal_tag'])\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY,testtimes,selector = get_train_test_bird(data_df,trn='LB0x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_df[data_df['animal_tag']==birds[selector]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainX(shape): ',trainX.shape)\n",
    "print('trainY(shape): ',trainY.shape)\n",
    "\n",
    "print('testX(shape): ',testX.shape)\n",
    "print('testY(shape): ',testY.shape)\n",
    "print('testtimes(len): ',len(testtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX , testX = timeseries_standardize(trainX, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclass_df = pd.read_csv(r'G:\\JaimeMorales\\Codes\\omizunagidori\\database\\labels\\O_labels_df.csv')\n",
    "simclass_d = simclass_df['label'].to_dict()\n",
    "#simclass_d[len('simclass_d')]='pad_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclass_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict1, testYYY, testXXX, Confidences, Ttimes, Sloss = ApplyUnet(trainX, trainY, testX, testY, testtimes, simclass_d, nb_epoch=200, fi=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred = pd.DataFrame(data = {'time':Ttimes, 'l_val':testYYY, 'predicted_label':predict1,'confidence':Confidences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = r'G:\\JaimeMorales\\Codes\\Biodata\\database\\results'\n",
    "save_fn = 'res_'+birds[selector]+'.csv'\n",
    "save_path = os.path.join(save_folder,save_fn)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred.to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tttimes = range(len(predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels,indices = get_activity_index(simclass_d)\n",
    "indices.append(11)\n",
    "labels.append('EPadding')\n",
    "figs = plt.figure('result',figsize=(10, 4))\n",
    "axs = plt.subplot(111)\n",
    "axs.plot(tttimes, predict1, color='r')\n",
    "axs.plot(tttimes, testYYY, color='g')\n",
    "axs.set_ylabel('Activity label')\n",
    "axs.set_yticks(indices)\n",
    "axs.set_yticklabels(labels)\n",
    "figs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
