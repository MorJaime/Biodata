{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer,concatenate, Conv2D, BatchNormalization, Concatenate, UpSampling2D, MaxPool2D, Input, Dropout, Dense, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import HAR_utils\n",
    "from HAR_utils import get_activity_index, plot_confusion_matrix, get_Train_Test, timeseries_standardize\n",
    "from attention import Attention2D\n",
    "from getattention import get_intermediate_output, write_intermediate_output\n",
    "\n",
    "fig_width = 12\n",
    "plt.rcParams[\"font.size\"] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_03_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_04_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_05_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_06_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(['LW_x','LW_y','LW_z'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY,testtimes,selector = get_Train_Test(data_df,trn=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_df[data_df['job']==selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainX(shape): ',trainX.shape)\n",
    "print('trainY(shape): ',trainY.shape)\n",
    "\n",
    "print('testX(shape): ',testX.shape)\n",
    "print('testY(shape): ',testY.shape)\n",
    "print('testtimes(len): ',len(testtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = trainX[[5]]\n",
    "trx = np.delete(trainX,5,0)\n",
    "print('trainX(shape): ',tex.shape)\n",
    "print('trainX(shape): ',trx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX , testX = timeseries_standardize(trainX, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(ip,filters=10):\n",
    "    depth_k = 10\n",
    "    depth_v = 10\n",
    "    num_heads = 1\n",
    "    qkv_conv = Conv2D(2 * depth_k + depth_v, (1, 1), strides=(1, 1), padding='same', use_bias=True, kernel_initializer='he_normal', name=\"pre-qkvcon\")(ip)\n",
    "    attn_out = Attention2D(depth_k, depth_v, num_heads, True, name=\"attn\")(qkv_conv)\n",
    "    attn_out = Conv2D(depth_v, (1, 1), strides=(1, 1), padding='same', use_bias=True, kernel_initializer='he_normal', name=\"pos-qkvcon\")(attn_out)\n",
    "    att_output = Multiply()([ip, attn_out])\n",
    "    att_output = BatchNormalization()(att_output)\n",
    "    return att_output\n",
    "\n",
    "def down_block(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"down_\"):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    p = MaxPool2D((1, 2),name=name+\"M\")(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"up_\"):\n",
    "    us = UpSampling2D((1, 2),name=name+\"U\")(x)\n",
    "    concat = Concatenate()([us, skip])\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(concat)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl2\")(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    return c\n",
    "\n",
    "def build_unet_model(height,channels,class_num,f=32):\n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    att1 = attention_block(c1, filters=f)\n",
    "    pa = MaxPool2D((1, 2))(att1)\n",
    "    c2, p2 = down_block(pa, f*2, name=\"down_2_\")\n",
    "    c3, p3 = down_block(p2, f*4, name=\"down_3_\")\n",
    "    p3 = Dropout(0.5)(p3)\n",
    "    \n",
    "    bn = bottleneck(p3, f*8)\n",
    "    \n",
    "    u1 = up_block(bn, c3, f*4, name=\"up_1_\")\n",
    "    u2 = up_block(u1, c2, f*2, name=\"up_2_\")\n",
    "    u3 = up_block(u2, att1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def ApplyUnet(trainX, trainY, testX, testY,TestTimes, activities, nb_epoch=100, fi=32):\n",
    "    \n",
    "    target_names,indices = get_activity_index(activities)\n",
    "    #print(target_names)\n",
    "    indices.append(11)\n",
    "    target_names.append('EPadding')\n",
    "    \n",
    "    trainY_b = np_utils.to_categorical(trainY, len(target_names))\n",
    "    testY_b = np_utils.to_categorical(testY, len(target_names))\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    print(indices)\n",
    "    lb.fit(indices)\n",
    "    \n",
    "    model = build_unet_model(trainX.shape[2],trainX.shape[3],len(target_names),f=fi)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    \n",
    "    #trainX = tf.convert_to_tensor(trainX, dtype=tf.float32)\n",
    "    #trainY_b = tf.convert_to_tensor(trainY_b, dtype=tf.float32)\n",
    "    def scheduler(epoch,lr):\n",
    "        if epoch < 20:\n",
    "            return lr\n",
    "        else:\n",
    "            return float(lr)\n",
    "        \n",
    "    callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "    history = model.fit(trainX, trainY_b, epochs=nb_epoch, validation_split=0.2, batch_size = 2, callbacks=[callback])\n",
    "    \n",
    "    fig, (tr_plt,cm_plt) = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
    "    \n",
    "    tr_plt.plot(history.history['loss'])\n",
    "    #tr_plt.plot(history.history['val_loss'])\n",
    "    tr_plt.set_title('Model loss')\n",
    "    tr_plt.set_ylabel('Loss')\n",
    "    tr_plt.set_xlabel('Epoch')\n",
    "    tr_plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "      \n",
    "    predict_v = model.predict(testX)\n",
    "    predict_vec = predict_v[0][0]\n",
    "    confidences = np.max(predict_vec, axis=1)\n",
    "    testYY = testY[0][0]\n",
    "    \n",
    "    predict_class = np.argmax(predict_vec, axis=1)\n",
    "    predict = [lb.classes_[x] for x in predict_class]\n",
    "    \n",
    "    print(\"len(testYY)\",len(testYY))\n",
    "    print(\"len(predict)\",len(predict))\n",
    "    \n",
    "    testXX = testX[0][0]\n",
    "    \n",
    "    '''\n",
    "    for i in range(len(testYY)):\n",
    "        if testYY[i]==11:\n",
    "            testYYY = testYY[:i]\n",
    "            predict1 = predict[:i]\n",
    "            testXXX = testXX[:i]\n",
    "            ttimes = TestTimes[:i]\n",
    "            Confidences = confidences[:i]\n",
    "            print(i)\n",
    "            break\n",
    "    \n",
    "    '''\n",
    "    testYYY = testYY\n",
    "    predict1 = predict\n",
    "    testXXX = testXX\n",
    "    ttimes = TestTimes\n",
    "    Confidences = confidences\n",
    "    \n",
    "    \n",
    "    print(\"len(testYYY)\",len(testYYY))\n",
    "    print(\"len(predict1)\",len(predict1))\n",
    "    \n",
    "    result = classification_report(testYYY, predict1, labels=indices, target_names=target_names)\n",
    "    print(result)\n",
    "    testXX=testX\n",
    "    cnf_matrix = confusion_matrix(testYYY, predict1)\n",
    "    plot_confusion_matrix(cnf_matrix, fig, cm_plt, classes=target_names)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "    return predict1, testYYY, testXXX, Confidences, ttimes, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclass_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\TF-env\\projects\\U-net\\acc2\\labels.csv')\n",
    "simclass_d = simclass_df['activity'].to_dict()\n",
    "#simclass_d[11]='pad_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict1, testYYY, testXXX, Confidences, Ttimes, model = ApplyUnet(trainX, trainY, testX, testY, testtimes, simclass_d, nb_epoch=100, fi=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_files = []\n",
    "for i in range(len(trainX)):\n",
    "    file = 'layer_output_res_4_1_' + str(i) + '.csv'\n",
    "    normal_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_intermediate_output(model, trainX, 4, normal_files, r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\results\\Test-17-08\\Self-Att-con\\Layer_values', Ttimes, layers=['attn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred = pd.DataFrame(data = {'time':Ttimes, 'l_val':testYYY, 'predicted_label':predict1,'confidence':Confidences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data_with_conf_pred[test_data_with_conf_pred['predicted_label']==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred.to_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\results\\Test-17-08\\Self-Att-con\\6\\res_6_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tttimes = range(len(predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,indices = get_activity_index(simclass_d)\n",
    "indices.append(11)\n",
    "labels.append('EPadding')\n",
    "figs = plt.figure('result',figsize=(10, 4))\n",
    "axs = plt.subplot(111)\n",
    "axs.plot(tttimes, predict1, color='r')\n",
    "axs.plot(tttimes, testYYY, color='g')\n",
    "axs.set_ylabel('Activity label')\n",
    "axs.set_yticks(indices)\n",
    "axs.set_yticklabels(labels)\n",
    "figs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
