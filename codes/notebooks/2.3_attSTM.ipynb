{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer, Conv2D, BatchNormalization, Concatenate, UpSampling2D, MaxPool2D, Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import HAR_utils\n",
    "from HAR_utils import get_activity_index, plot_confusion_matrix, get_Train_Test, timeseries_standardize\n",
    "from attention import Attention2D\n",
    "\n",
    "fig_width = 12\n",
    "plt.rcParams[\"font.size\"] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  []\n"
     ]
    }
   ],
   "source": [
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_utils import num_labels, time_change, setup_dir, find_xml_filenames, find_csv_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omizu_path = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\export\\Omizunagidori\"\n",
    "umineko_path = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\export\\Umineko\"\n",
    "database_path = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\"\n",
    "database_l_path = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\labels\"\n",
    "database_o_acc_path = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\omizunagidori\"\n",
    "database_u_acc_path = r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\umineko\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Jaime\\\\Documents\\\\JaimeMorales\\\\Codes\\\\HAR_acc\\\\acc2\\\\daily_data\\\\acc_03_r.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b0366530c90d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\daily_data\\acc_03_r.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Jaime\\\\Documents\\\\JaimeMorales\\\\Codes\\\\HAR_acc\\\\acc2\\\\daily_data\\\\acc_03_r.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"G:\\JaimeMorales\\Codes\\omizunagidori\\database\\omizunagidori\\2022\\l_2022_acc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(['LW_x','LW_y','LW_z'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_index(act_dict):\n",
    "    labels = []\n",
    "    indices = []\n",
    "    for k,v in act_dict.items():\n",
    "        indices.append(k)\n",
    "        labels.append(v)\n",
    "    return labels, indices\n",
    "\n",
    "def check_dict(dic):\n",
    "    new_dic = dic\n",
    "    rl = []\n",
    "    vlm = int(np.mean(list(dic.values()))/10)\n",
    "    print('vlm',vlm)\n",
    "    for key,value in dic.items():\n",
    "        if value <= vlm:\n",
    "            rl.append(key)\n",
    "    for k in rl:\n",
    "        new_dic.pop(k)\n",
    "    return new_dic\n",
    "\n",
    "def balance_data(X, Y, activities, ratio_to_min=1.2):\n",
    "    #XをUnder sampling\n",
    "    #https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler\n",
    "    target_num_dictionary = {}#クラスごとに所望のインスタンス数を設定\n",
    "    print(\"before balancing\")\n",
    "    for key, value in activities.items():#行動ごとにインスタンス数をカウント\n",
    "        instance_num = np.sum(Y == key, axis=0)\n",
    "        print(\"%s: %i\"% (key,instance_num))\n",
    "        target_num_dictionary[key] = instance_num\n",
    "    #print(target_num_dictionary.items())\n",
    "    target_num_dictionary=check_dict(target_num_dictionary)\n",
    "    #print(target_num_dictionary.items())\n",
    "    \n",
    "    min_instance_num = min(target_num_dictionary.values())#最小のインスタンス数\n",
    "    \n",
    "    for key, value in target_num_dictionary.items():\n",
    "        if value > min_instance_num:\n",
    "            target_num_dictionary[key] = min(value, int(min_instance_num*ratio_to_min))#最小のインスタンス数でない場合は、最小のratio_to_min倍にするよう設定\n",
    "    rus = RandomUnderSampler(sampling_strategy=target_num_dictionary)#Under samplerの構築\n",
    "    X_resampled, Y_resampled = rus.fit_sample(X, Y)#under sampling実行\n",
    "\n",
    "    print(\"after balancing\")\n",
    "    for key, value in activities.items():\n",
    "        print(\"%s: %i\"% (key,np.sum(Y_resampled == key, axis=0)))\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "def raw_to_nparray(data, winsize, skip, activities, shuffle=False, undersample=False, us_ratio=1.2):\n",
    "    fp = FloatProgress(min=0, max=len(data)-winsize)\n",
    "    fp.value = 0\n",
    "    display(fp)\n",
    "    fp.description = \"converting to np.array\"\n",
    "    \n",
    "    features = data.columns[1:-2]#データ系列名のリスト。最初（time）と最後（label）以外\n",
    "    print(features)\n",
    "    X = []#データ用リスト\n",
    "    Y = []#ラベル用リスト\n",
    "    times = []#タイムスタンプ用リスト\n",
    "    for i in range(0,len(data)-winsize,skip):#skipだけずらしてウインドウを生成\n",
    "        window = data[features][i:i+winsize]#winsize幅のウインドウ\n",
    "        \n",
    "        cidx = i + int(winsize/2)#ウインドウの中心のインデックス\n",
    "        X.append(window.values)#データをndarrayにしてリストに追加 (ウインドウ幅,次元数)の形状をもつ。\n",
    "        Y.append(data[\"l_val\"][cidx])#ウインドウの中心のラベルをリストに追加\n",
    "        times.append(data[\"time\"][cidx])#ウインドウの中心の時刻をリストに追加\n",
    "        \n",
    "        fp.value = fp.value + skip\n",
    "    \n",
    "    if undersample: #Undersamplingする場合\n",
    "        timesteps = X[0].shape[0]\n",
    "        ndim = X[0].shape[1]\n",
    "        for i in range(len(X)):\n",
    "            X[i] = X[i].ravel()#次の行の関数のために2次元のndarrayにする必要があるため、各データを一次元に変換\n",
    "        X_, Y = balance_data(np.array(X), np.array(Y), activities, us_ratio)\n",
    "        X_ = X_.reshape(X_.shape[0], timesteps, ndim)#(インスタンス数,ウインドウ幅,次元数)の形状にする\n",
    "        Y = Y.tolist()\n",
    "        X = []#リストの形式に戻す(以降でシャッフルするため)\n",
    "        for idx in range(len(Y)):\n",
    "            X.append(X_[idx,:,:])\n",
    "        times = []#時刻は意味をなさなくなったため削除\n",
    "        \n",
    "    if shuffle:#シャッフルする場合\n",
    "        p = list(zip(X,Y))#2つのリストのiterator\n",
    "        random.shuffle(p)#シャッフル\n",
    "        X, Y = zip(*p)#２つのリストに反映\n",
    "        X = list(X)#タプル形式をリストに戻す\n",
    "        Y = list(Y)\n",
    "        times = []#時刻は意味をなさなくなったため削除\n",
    "        \n",
    "    return np.array(X), Y, times\n",
    "\n",
    "def timeseries_standardize(trainX, testX):\n",
    "\n",
    "    ss = preprocessing.StandardScaler()#標準化のためのモジュール\n",
    "    trainXshape = trainX.shape #元のshapeを保持\n",
    "    trainX = trainX.reshape(-1, trainX.shape[2]) #2次元のndarrayに変換。(-1,次元数)のshape\n",
    "    ss.fit(trainX)#学習データから、各次元ごとの平均と分散を計算\n",
    "    trainX = ss.transform(trainX) #標準化\n",
    "    trainX = trainX.reshape(trainXshape) #元のshapeに戻す\n",
    "\n",
    "    testXshape = testX.shape#元のshapeを保持\n",
    "    testX = testX.reshape(-1, testX.shape[2]) #2次元のndarrayに変換。(-1,次元数)のshape\n",
    "    testX = ss.transform(testX) #標準化\n",
    "    testX = testX.reshape(testXshape) #元のshapeに戻す\n",
    "    \n",
    "    return trainX, testX\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "from attention1 import AttentionWithContext\n",
    "\n",
    "def build_model(input_dim,timesteps,class_num):\n",
    "    hidden_unit = 20\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_unit, activation=\"sigmoid\", input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(hidden_unit, activation=\"sigmoid\", return_sequences=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(hidden_unit, activation=\"sigmoid\"))\n",
    "    #model.add(Dense(hidden_unit, activation='tanh'))\n",
    "    model.add(Dense(class_num, activation='softmax'))\n",
    "    \n",
    "    #opt = keras.optimizers.Adam(learning_rate=0.05)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def ApplyLSTMNetwork(trainX, trainY, testX, testY, activities,nb_epoch = 100, batch_size=32):\n",
    "    target_names,indices = get_activity_index(activities)\n",
    "    #Yをone-hot encodingに変換\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(indices)\n",
    "    trainY_b = lb.transform(trainY)\n",
    "    testY_b = lb.transform(testY)\n",
    "    print(trainY_b)\n",
    "    #モデルを構築\n",
    "    model = build_model(trainX.shape[2], trainX.shape[1], len(target_names))\n",
    "    model.summary()\n",
    "    #学習・推定\n",
    "    #エポック数\n",
    "    #plot_losses = keras_utils.Callbacks() #lossの可視化\n",
    "    #model.fit(trainX, trainY_b, nb_epoch=nb_epoch, validation_split=0.2, callbacks=[plot_losses], verbose=0)#学習\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint('lstm_membrane.hdf5', monitor='loss',verbose=0, save_best_only=True)\n",
    "    history = model.fit(trainX, trainY_b, epochs=nb_epoch, validation_split=0.2,callbacks=[model_checkpoint], verbose=1, batch_size=batch_size)\n",
    "    #history = model.fit(trainX, trainY_b, nb_epoch=nb_epoch, validation_split=0.2, verbose=0)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #モデル概要\n",
    "    img = SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))#モデルの可視化\n",
    "    display(img)\n",
    "    predict_vec = model.predict(testX)#推定\n",
    "    predict_class = np.argmax(predict_vec, axis=1)#推定結果をクラスのインデックスの表現（0,1,2,...,len(class_num)）になおす\n",
    "    predict = [lb.classes_[x] for x in predict_class] #元のラベル（数値）に戻す\n",
    "    print(predict_class)\n",
    "\n",
    "    #分類結果\n",
    "    result = classification_report(testY, predict,labels=indices, target_names=target_names)\n",
    "    print(result)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    cnf_matrix = confusion_matrix(testY, predict)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "\n",
    "    return predict_vec, lb.classes_\n",
    "\n",
    "def Train_Test_sep(inn_df,sel=100):\n",
    "    \n",
    "    df_len = 0\n",
    "    \n",
    "    while df_len == 0:\n",
    "        if sel == 100:\n",
    "            selector = random.randint(0,max(list(inn_df['job'])))\n",
    "        else:\n",
    "            selector = sel\n",
    "        \n",
    "        testdata_df = inn_df[inn_df['job']==selector]\n",
    "        testdata_df.reset_index(inplace=True)\n",
    "        testdata_df.drop(['job','index'],axis=1,inplace=True)\n",
    "    \n",
    "        traindata_df = inn_df[inn_df['job']!=selector]\n",
    "        traindata_df.reset_index(inplace=True)\n",
    "        traindata_df.drop(['job','index'],axis=1,inplace=True)\n",
    "        df_len = len(testdata_df)\n",
    "        print('df_len: ', df_len)\n",
    "    \n",
    "    \n",
    "    return testdata_df,traindata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_val(cur_acc_dir, save_dir, winsize, skip, lab_d, crossval=10, sels=[1000], nb_epoch = 100, batch_size=32):\n",
    "    \n",
    "    cur_acc_df = pd.read_csv(cur_acc_dir)\n",
    "    act_day = cur_acc_dir[-8:-6]\n",
    "    newpath = os.path.join(save_dir, act_day)\n",
    "    \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "    cur_acc_df.drop(['LW_x','LW_y','LW_z'],axis=1,inplace=True)\n",
    "    \n",
    "    if sels[0] == 1000:\n",
    "        sel_l = [100]*crossval\n",
    "    else:\n",
    "        sel_l = sels\n",
    "    \n",
    "    for i in range(len(sel_l)):\n",
    "        print(i)\n",
    "        sel = sel_l[i]\n",
    "        \n",
    "        save_str = 'res_' + act_day + '_' + str(i) + '.csv'\n",
    "        path_dir = os.path.join(newpath, save_str)\n",
    "        \n",
    "        test_data,train_data = Train_Test_sep(cur_acc_df,sel)\n",
    "    \n",
    "        trainX, trainY,_ = raw_to_nparray(train_data, winsize, skip, lab_d, True, True,us_ratio=1.5)\n",
    "        testX, testY, testTimes = raw_to_nparray(test_data, winsize, skip, lab_d)\n",
    "        trainX, testX = timeseries_standardize(trainX, testX)\n",
    "    \n",
    "        predict_vec, classes_ = ApplyLSTMNetwork(trainX, trainY, testX, testY, lab_d, nb_epoch, batch_size)\n",
    "    \n",
    "        confidences = np.max(predict_vec, axis=1) \n",
    "        predict = [classes_[x] for x in np.argmax(predict_vec, axis=1)]\n",
    "        test_data_with_conf_pred = pd.merge(test_data, pd.DataFrame(data = {'time':testTimes, 'predicted_label':predict,'confidence':confidences}), on='time')\n",
    "    \n",
    "        start_time = test_data_with_conf_pred['time'][0]\n",
    "        end_time = test_data_with_conf_pred['time'][len(test_data_with_conf_pred)-1]\n",
    "        Ttimes = list(test_data_with_conf_pred['time'])\n",
    "        predict1 = list(test_data_with_conf_pred['predicted_label'])\n",
    "        testYYY = list(test_data_with_conf_pred['l_val'])\n",
    "    \n",
    "        labels,indices = get_activity_index(lab_d)\n",
    "        figs = plt.figure('image',figsize=(16, 6))\n",
    "        axs = plt.subplot(111)\n",
    "        axs.plot(Ttimes, predict1, color='r',label='Prediction')\n",
    "        axs.plot(Ttimes, testYYY, color='g',label='Ground truth')\n",
    "        axs.set_ylabel('Activity label')\n",
    "        axs.set_xlabel('time[ms]')\n",
    "        axs.set_yticks(indices)\n",
    "        axs.set_yticklabels(labels)\n",
    "        axs.legend(loc='upper right')\n",
    "        figs.show()\n",
    "    \n",
    "        test_data_with_conf_pred.to_csv(path_dir,index=False)\n",
    "        print(path_dir)\n",
    "        print('end'+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY,testtimes,selector = get_Train_Test(data_df,trn=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_df[data_df['job']==selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainX(shape): ',trainX.shape)\n",
    "print('trainY(shape): ',trainY.shape)\n",
    "\n",
    "print('testX(shape): ',testX.shape)\n",
    "print('testY(shape): ',testY.shape)\n",
    "print('testtimes(len): ',len(testtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = trainX[[5]]\n",
    "trx = np.delete(trainX,5,0)\n",
    "print('trainX(shape): ',tex.shape)\n",
    "print('trainX(shape): ',trx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX , testX = timeseries_standardize(trainX, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"down_\"):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    p = MaxPool2D((1, 2),name=name+\"M\")(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(1, 3), padding=\"same\", strides=1, name=\"up_\"):\n",
    "    us = UpSampling2D((1, 2),name=name+\"U\")(x)\n",
    "    concat = Concatenate()([us, skip])\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"1\")(concat)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=name+\"2\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(1, 3), padding=\"same\", strides=1):\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl1\")(x)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\",name=\"btl2\")(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    return c\n",
    "\n",
    "def build_unet_model(height,channels,class_num,f=32):\n",
    "    \n",
    "    inputs = Input((1, height, channels))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f, name=\"down_1_\")\n",
    "    c2, p2 = down_block(p1, f*2, name=\"down_2_\")\n",
    "    c3, p3 = down_block(p2, f*4, name=\"down_3_\")\n",
    "    p3 = Dropout(0.5)(p3)\n",
    "    \n",
    "    bn = bottleneck(p3, f*8)\n",
    "    \n",
    "    u1 = up_block(bn, c3, f*4, name=\"up_1_\")\n",
    "    u2 = up_block(u1, c2, f*2, name=\"up_2_\")\n",
    "    u3 = up_block(u2, c1, f, name=\"up_3_\")\n",
    "    \n",
    "    outputs = Dense(class_num, activation = 'softmax')(u3)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def ApplyUnet(trainX, trainY, testX, testY,TestTimes, activities, nb_epoch=100, fi=32):\n",
    "    \n",
    "    target_names,indices = get_activity_index(activities)\n",
    "    #print(target_names)\n",
    "    indices.append(11)\n",
    "    target_names.append('EPadding')\n",
    "    \n",
    "    trainY_b = np_utils.to_categorical(trainY, len(target_names))\n",
    "    testY_b = np_utils.to_categorical(testY, len(target_names))\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    print(indices)\n",
    "    lb.fit(indices)\n",
    "    \n",
    "    model = build_unet_model(trainX.shape[2],trainX.shape[3],len(target_names),f=fi)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "    history = model.fit(trainX, trainY_b, epochs=nb_epoch, validation_split=0.2,callbacks=[model_checkpoint], verbose=0)\n",
    "    \n",
    "    fig, (tr_plt,cm_plt) = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
    "    \n",
    "    tr_plt.plot(history.history['loss'])\n",
    "    tr_plt.plot(history.history['val_loss'])\n",
    "    tr_plt.set_title('Model loss')\n",
    "    tr_plt.set_ylabel('Loss')\n",
    "    tr_plt.set_xlabel('Epoch')\n",
    "    tr_plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "      \n",
    "    predict_v = model.predict(testX)\n",
    "    predict_vec = predict_v[0][0]\n",
    "    confidences = np.max(predict_vec, axis=1)\n",
    "    testYY = testY[0][0]\n",
    "    \n",
    "    predict_class = np.argmax(predict_vec, axis=1)\n",
    "    predict = [lb.classes_[x] for x in predict_class]\n",
    "    \n",
    "    print(\"len(testYY)\",len(testYY))\n",
    "    print(\"len(predict)\",len(predict))\n",
    "    \n",
    "    testXX = testX[0][0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(len(testYY)):\n",
    "        if testYY[i]==11:\n",
    "            testYYY = testYY[:i]\n",
    "            predict1 = predict[:i]\n",
    "            testXXX = testXX[:i]\n",
    "            ttimes = TestTimes[:i]\n",
    "            Confidences = confidences[:i]\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "    \n",
    "    '''\n",
    "    testYYY = testYY\n",
    "    predict1 = predict\n",
    "    testXXX = testXX\n",
    "    ttimes = TestTimes\n",
    "    Confidences = confidences\n",
    "    '''\n",
    "              \n",
    "    print(\"len(testYYY)\",len(testYYY))\n",
    "    print(\"len(predict1)\",len(predict1))\n",
    "    \n",
    "    result = classification_report(testYYY, predict1, labels=indices, target_names=target_names)\n",
    "    print(result)\n",
    "    testXX=testX\n",
    "    cnf_matrix = confusion_matrix(testYYY, predict1)\n",
    "    plot_confusion_matrix(cnf_matrix, fig, cm_plt, classes=target_names)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    Sloss = history.history['loss']\n",
    "\n",
    "    return predict1, testYYY, testXXX, Confidences, ttimes, Sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclass_df = pd.read_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\TF-env\\projects\\U-net\\acc2\\labels.csv')\n",
    "simclass_d = simclass_df['activity'].to_dict()\n",
    "#simclass_d[11]='pad_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict1, testYYY, testXXX, Confidences, Ttimes, Sloss = ApplyUnet(trainX, trainY, testX, testY, testtimes, simclass_d, nb_epoch=100, fi=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred = pd.DataFrame(data = {'time':Ttimes, 'l_val':testYYY, 'predicted_label':predict1,'confidence':Confidences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_conf_pred.to_csv(r'C:\\Users\\Jaime\\Documents\\JaimeMorales\\Codes\\HAR_acc\\acc2\\results\\Test-03-09\\U-net\\6\\res_6_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tttimes = range(len(predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels,indices = get_activity_index(simclass_d)\n",
    "indices.append(11)\n",
    "labels.append('EPadding')\n",
    "figs = plt.figure('result',figsize=(10, 4))\n",
    "axs = plt.subplot(111)\n",
    "axs.plot(tttimes, predict1, color='r')\n",
    "axs.plot(tttimes, testYYY, color='g')\n",
    "axs.set_ylabel('Activity label')\n",
    "axs.set_yticks(indices)\n",
    "axs.set_yticklabels(labels)\n",
    "figs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
